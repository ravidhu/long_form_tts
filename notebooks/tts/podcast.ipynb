{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Podcast Pipeline Testing\n",
    "\n",
    "End-to-end podcast pipeline: PDF → section extraction → LLM outline → LLM dialogue → TTS rendering.\n",
    "\n",
    "**Prerequisites:** `uv sync --extra mlx` (Kokoro + Chatterbox).\n",
    "\n",
    "**What you can do here:**\n",
    "- Extract sections from a PDF and inspect them\n",
    "- Generate a podcast outline from all sections\n",
    "- Generate two-speaker dialogue for a section\n",
    "- Render dialogue to audio with Kokoro or Chatterbox TTS\n",
    "- Compare backends side by side\n",
    "- Save rendered audio to WAV files\n",
    "\n",
    "Run cells 1–2 (setup + config) first, then run sections in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# 1. Setup — add src/ to path so we can import all packages\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parents[1] / \"src\"))\n",
    "\n",
    "from shared.extract import extract_sections\n",
    "from shared.providers import (\n",
    "    KokoroTTS, ChatterboxTTS,\n",
    "    MLXLLM, OllamaLLM,\n",
    "    get_tts_runtime,\n",
    ")\n",
    "from podcast import (\n",
    "    PodcastConfig, DialogueConfig,\n",
    "    generate_outline, generate_dialogue_segment, generate_intro_outro,\n",
    "    load_tts_model, render_dialogue, get_sample_rate,\n",
    ")\n",
    "\n",
    "print(f\"TTS runtime: {get_tts_runtime()}\")\n",
    "print(\"Imports OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper — render audio and show inline player with stats\n",
    "\n",
    "def play(audio: np.ndarray, sr: int, label: str = \"\"):\n",
    "    \"\"\"Display an inline audio player with duration stats.\"\"\"\n",
    "    duration = len(audio) / sr\n",
    "    prefix = f\"{label}: \" if label else \"\"\n",
    "    print(f\"{prefix}{duration:.1f}s, {len(audio):,} samples @ {sr} Hz\")\n",
    "    display(Audio(audio, rate=sr))\n",
    "\n",
    "print(\"play() helper defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Configuration\n",
    "\n",
    "Set your LLM backend, TTS backend, dialogue format, and extraction parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Pipeline configuration\n",
    "#\n",
    "# Language: source_lang and target_lang propagate to the entire pipeline:\n",
    "#   - LLM prompts are automatically instructed to write in target_lang\n",
    "#   - TTS voices are auto-selected from target_lang if tts= is omitted\n",
    "#\n",
    "# If you set tts= explicitly, make sure voices match target_lang.\n",
    "# e.g. target_lang=\"fr\" needs French voices (ff_siwis), not English (bf_emma).\n",
    "\n",
    "config = PodcastConfig(\n",
    "    dialogue=DialogueConfig(\n",
    "        format=\"host_guest\",           # \"two_hosts\" | \"host_guest\" — see docs/api_reference.md#dialogue-formats\n",
    "        speaker1_name=\"Alex\",\n",
    "        speaker2_name=\"Sam\",\n",
    "        source_lang=\"en\",\n",
    "        target_lang=\"en\",\n",
    "        segment_target_words=1200,    # ~8 min per segment\n",
    "        words_per_minute=150,\n",
    "    ),\n",
    "    # LLM — uncomment one\n",
    "    # llm=MLXLLM(model=\"Qwen/Qwen3-14B-MLX-4bit\"),\n",
    "    llm=OllamaLLM(model=\"qwen3:14b\", num_ctx=40960, temperature=0.7),\n",
    "\n",
    "    # TTS — uncomment one (or omit to auto-select from target_lang)\n",
    "    tts=KokoroTTS(voices=(\"bf_emma\", \"bm_george\"), speeds=(1.0, 1.2)),\n",
    "    # tts=KokoroTTS(lang=\"fr\"),  # auto-selects French voices\n",
    "    # tts=ChatterboxTTS(audio_prompts=(\"voices/host.wav\", \"voices/guest.wav\")),\n",
    ")\n",
    "\n",
    "# Extraction settings\n",
    "MAX_TOC_LEVEL = 1       # 1 = chapters, 2 = sub-chapters\n",
    "CONTEXT_BUDGET = 20_000  # max tokens per section\n",
    "PDF_BACKEND = \"pymupdf\"  # \"pymupdf\" or \"docling\"\n",
    "\n",
    "dlg = config.dialogue\n",
    "print(f\"Format: {dlg.format} ({dlg.speaker1_name} & {dlg.speaker2_name})\")\n",
    "print(f\"LLM: {type(config.llm).__name__} / {config.llm.model}\")\n",
    "print(f\"TTS: {type(config.tts).__name__}\")\n",
    "print(f\"Language: {dlg.source_lang} → {dlg.target_lang}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Extract Sections from PDF\n",
    "\n",
    "Runs the TOC analysis and per-section markdown extraction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Extract sections from a PDF\n",
    "#     Downloads the Adam optimizer paper (Kingma & Ba, 2015) from arxiv.\n",
    "#     Cached locally after first download.\n",
    "\n",
    "source = \"https://arxiv.org/pdf/1412.6980\"\n",
    "# source = \"../../inputs/your-book.pdf\"         # or use a local PDF\n",
    "# source = \"https://example.com/article\"        # or a webpage URL\n",
    "\n",
    "print(f\"Source: {source}\")\n",
    "print(f\"TOC level: {MAX_TOC_LEVEL}, budget: {CONTEXT_BUDGET:,} tokens\\n\")\n",
    "\n",
    "t0 = time.time()\n",
    "raw_sections = extract_sections(\n",
    "    source,\n",
    "    max_toc_level=MAX_TOC_LEVEL,\n",
    "    context_budget=CONTEXT_BUDGET,\n",
    "    backend=PDF_BACKEND,\n",
    ")\n",
    "elapsed = time.time() - t0\n",
    "\n",
    "print(f\"\\nExtracted {len(raw_sections)} sections in {elapsed:.1f}s:\\n\")\n",
    "for i, (title, content) in enumerate(raw_sections):\n",
    "    chars = len(content)\n",
    "    tokens_est = chars // 4\n",
    "    print(f\"  {i+1}. {title} ({chars:,} chars, ~{tokens_est:,} tokens)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. LLM Outline Generation\n",
    "\n",
    "Generate a podcast outline from all extracted sections. This guides the dialogue generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Generate podcast outline\n",
    "\n",
    "print(f\"Generating outline from {len(raw_sections)} sections...\")\n",
    "print(f\"LLM: {type(config.llm).__name__} / {config.llm.model}\\n\")\n",
    "\n",
    "t0 = time.time()\n",
    "outline = generate_outline(raw_sections, config)\n",
    "elapsed = time.time() - t0\n",
    "\n",
    "print(f\"Outline generated in {elapsed:.1f}s\")\n",
    "print(f\"Episode title: {outline.title}\")\n",
    "print(\"=\" * 60)\n",
    "print(outline.raw_text[:2000])\n",
    "if len(outline.raw_text) > 2000:\n",
    "    print(f\"\\n... ({len(outline.raw_text) - 2000:,} more chars)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. LLM Dialogue Generation\n",
    "\n",
    "Generate two-speaker dialogue for one section, using the outline for context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Generate dialogue for one section\n",
    "\n",
    "section_idx = 0  # change this to generate dialogue for different sections\n",
    "title, content = raw_sections[section_idx]\n",
    "\n",
    "dlg = config.dialogue\n",
    "print(f\"Generating dialogue for: {title}\")\n",
    "print(f\"  Target: ~{dlg.segment_target_words} words (~{dlg.segment_target_words // dlg.words_per_minute} min)\")\n",
    "print(f\"  LLM: {type(config.llm).__name__} / {config.llm.model}\\n\")\n",
    "\n",
    "t0 = time.time()\n",
    "result = generate_dialogue_segment(\n",
    "    section_content=content,\n",
    "    section_title=title,\n",
    "    outline=outline,\n",
    "    segment_index=section_idx,\n",
    "    rolling_summary=\"\",\n",
    "    covered_topics=[],\n",
    "    config=config,\n",
    ")\n",
    "elapsed = time.time() - t0\n",
    "\n",
    "dialogue = result.dialogue\n",
    "word_count = len(dialogue.split())\n",
    "est_minutes = word_count / dlg.words_per_minute\n",
    "\n",
    "print(f\"Dialogue generated in {elapsed:.1f}s\")\n",
    "print(f\"Output: {len(dialogue):,} chars, {word_count} words (~{est_minutes:.1f} min)\")\n",
    "print(f\"Topics covered: {len(result.covered_topics)}\")\n",
    "print(\"=\" * 60)\n",
    "print(dialogue[:3000])\n",
    "if len(dialogue) > 3000:\n",
    "    print(f\"\\n... ({len(dialogue) - 3000:,} more chars)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. TTS Rendering\n",
    "\n",
    "Render the dialogue to audio with the configured TTS backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Load TTS model and render\n",
    "\n",
    "sr = get_sample_rate(config.tts)\n",
    "\n",
    "print(f\"Loading {type(config.tts).__name__} model...\")\n",
    "t0 = time.time()\n",
    "model = load_tts_model(config.tts)\n",
    "print(f\"Model loaded in {time.time() - t0:.1f}s\")\n",
    "\n",
    "print(f\"\\nRendering {len(dialogue.split())} words...\")\n",
    "t0 = time.time()\n",
    "audio = render_dialogue(dialogue, config.tts, model=model)\n",
    "render_time = time.time() - t0\n",
    "\n",
    "print(f\"Render time: {render_time:.1f}s\")\n",
    "play(audio, sr=sr, label=\"Podcast\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Backend Comparison\n",
    "\n",
    "Load all three TTS backends and compare the same dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Load backends to compare\n",
    "backends = {\n",
    "    \"Kokoro\": KokoroTTS(voices=(\"bf_emma\", \"bm_george\")),\n",
    "    \"Chatterbox\": ChatterboxTTS(),\n",
    "}\n",
    "\n",
    "models = {}\n",
    "for name, tts_cfg in backends.items():\n",
    "    print(f\"{name}: {tts_cfg}\")\n",
    "    t0 = time.time()\n",
    "    models[name] = load_tts_model(tts_cfg)\n",
    "    print(f\"  Loaded in {time.time() - t0:.1f}s\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Render the same dialogue with all backends\n",
    "#     Uses the dialogue generated in section 4.\n",
    "\n",
    "results = {}\n",
    "for name, tts_cfg in backends.items():\n",
    "    sr = get_sample_rate(tts_cfg)\n",
    "    print(f\"Rendering with {name}...\")\n",
    "    t0 = time.time()\n",
    "    audio_out = render_dialogue(dialogue, tts_cfg, model=models[name])\n",
    "    elapsed = time.time() - t0\n",
    "    results[name] = (audio_out, elapsed, sr)\n",
    "\n",
    "# Stats table\n",
    "print(f\"\\n{'Metric':<20}\", end=\"\")\n",
    "for name in results:\n",
    "    print(f\" {name:>16}\", end=\"\")\n",
    "print()\n",
    "print(f\"{'-' * 20}\", end=\"\")\n",
    "for _ in results:\n",
    "    print(f\" {'-' * 16}\", end=\"\")\n",
    "print()\n",
    "\n",
    "print(f\"{'Render time (s)':<20}\", end=\"\")\n",
    "for audio_out, elapsed, sr in results.values():\n",
    "    print(f\" {elapsed:>16.1f}\", end=\"\")\n",
    "print()\n",
    "\n",
    "print(f\"{'Audio duration (s)':<20}\", end=\"\")\n",
    "for audio_out, elapsed, sr in results.values():\n",
    "    print(f\" {len(audio_out) / sr:>16.1f}\", end=\"\")\n",
    "print()\n",
    "\n",
    "# Play each\n",
    "for name, (audio_out, elapsed, sr) in results.items():\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    play(audio_out, sr=sr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "long-form-tts (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
