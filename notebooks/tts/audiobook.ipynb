{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Audiobook Pipeline Testing\n",
    "\n",
    "End-to-end audiobook pipeline: PDF → section extraction → LLM narration → TTS rendering.\n",
    "\n",
    "**Prerequisites:** `uv sync --extra mlx` (Kokoro + Chatterbox).\n",
    "\n",
    "**What you can do here:**\n",
    "- Extract sections from a PDF and inspect them\n",
    "- Generate LLM-adapted narration for a section\n",
    "- Render narration to audio with Kokoro or Chatterbox TTS\n",
    "- Compare backends side by side\n",
    "- Save rendered audio to WAV files\n",
    "\n",
    "Run cells 1–2 (setup + config) first, then run sections in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# 1. Setup — add src/ to path so we can import all packages\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parents[1] / \"src\"))\n",
    "\n",
    "from shared.extract import extract_sections\n",
    "from shared.markdown_parser import Section\n",
    "from shared.providers import (\n",
    "    KokoroTTS, ChatterboxTTS,\n",
    "    MLXLLM, OllamaLLM,\n",
    "    get_tts_runtime,\n",
    ")\n",
    "from audiobook import (\n",
    "    AudiobookConfig, NarrationConfig,\n",
    "    adapt_narration_section,\n",
    "    load_tts_model, render_section, SAMPLE_RATE,\n",
    ")\n",
    "\n",
    "print(f\"TTS runtime: {get_tts_runtime()}\")\n",
    "print(\"Imports OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper — render audio and show inline player with stats\n",
    "\n",
    "def play(audio: np.ndarray, sr: int = SAMPLE_RATE, label: str = \"\"):\n",
    "    \"\"\"Display an inline audio player with duration stats.\"\"\"\n",
    "    duration = len(audio) / sr\n",
    "    prefix = f\"{label}: \" if label else \"\"\n",
    "    print(f\"{prefix}{duration:.1f}s, {len(audio):,} samples @ {sr} Hz\")\n",
    "    display(Audio(audio, rate=sr))\n",
    "\n",
    "print(\"play() helper defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Configuration\n",
    "\n",
    "Set your LLM backend, TTS backend, and extraction parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Pipeline configuration\n",
    "#\n",
    "# Language: source_lang and target_lang propagate to the entire pipeline:\n",
    "#   - LLM prompts are automatically instructed to write in target_lang\n",
    "#   - TTS voices are auto-selected from target_lang if tts= is omitted\n",
    "#\n",
    "# If you set tts= explicitly, make sure the voice matches target_lang.\n",
    "# e.g. target_lang=\"fr\" needs a French voice (ff_siwis), not English (af_heart).\n",
    "\n",
    "config = AudiobookConfig(\n",
    "    narration=NarrationConfig(\n",
    "        source_lang=\"en\",\n",
    "        target_lang=\"en\",\n",
    "    ),\n",
    "    # LLM — uncomment one\n",
    "    llm=MLXLLM(model=\"Qwen/Qwen3-14B-MLX-4bit\"),\n",
    "    # llm=OllamaLLM(model=\"qwen3:14b\", num_ctx=40960, temperature=0.3),\n",
    "\n",
    "    # TTS — uncomment one (or omit to auto-select from target_lang)\n",
    "    tts=KokoroTTS(voices=(\"af_heart\",), speed=0.95),\n",
    "    # tts=KokoroTTS(lang=\"fr\"),  # auto-selects French voice\n",
    "    # tts=ChatterboxTTS(),\n",
    ")\n",
    "\n",
    "# Extraction settings\n",
    "MAX_TOC_LEVEL = 1       # 1 = chapters, 2 = sub-chapters\n",
    "CONTEXT_BUDGET = 20_000  # max tokens per section\n",
    "PDF_BACKEND = \"pymupdf\"  # \"pymupdf\" or \"docling\"\n",
    "\n",
    "print(f\"LLM: {type(config.llm).__name__} / {config.llm.model}\")\n",
    "print(f\"TTS: {type(config.tts).__name__}\")\n",
    "print(f\"Language: {config.narration.source_lang} → {config.narration.target_lang}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Extract Sections from PDF\n",
    "\n",
    "Runs the TOC analysis and per-section markdown extraction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Extract sections from a PDF\n",
    "#     Downloads the Adam optimizer paper (Kingma & Ba, 2015) from arxiv.\n",
    "#     Cached locally after first download.\n",
    "\n",
    "source = \"https://arxiv.org/pdf/1412.6980\"\n",
    "# source = \"../../inputs/your-book.pdf\"         # or use a local PDF\n",
    "# source = \"https://example.com/article\"        # or a webpage URL\n",
    "\n",
    "print(f\"Source: {source}\")\n",
    "print(f\"TOC level: {MAX_TOC_LEVEL}, budget: {CONTEXT_BUDGET:,} tokens\\n\")\n",
    "\n",
    "t0 = time.time()\n",
    "raw_sections = extract_sections(\n",
    "    source,\n",
    "    max_toc_level=MAX_TOC_LEVEL,\n",
    "    context_budget=CONTEXT_BUDGET,\n",
    "    backend=PDF_BACKEND,\n",
    ")\n",
    "elapsed = time.time() - t0\n",
    "\n",
    "print(f\"\\nExtracted {len(raw_sections)} sections in {elapsed:.1f}s:\\n\")\n",
    "for i, (title, content) in enumerate(raw_sections):\n",
    "    chars = len(content)\n",
    "    tokens_est = chars // 4\n",
    "    print(f\"  {i+1}. {title} ({chars:,} chars, ~{tokens_est:,} tokens)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Inspect a section\n",
    "\n",
    "section_idx = 0  # change this to inspect different sections\n",
    "title, content = raw_sections[section_idx]\n",
    "\n",
    "print(f\"Section {section_idx + 1}: {title}\")\n",
    "print(f\"Length: {len(content):,} chars, ~{len(content)//4:,} tokens\")\n",
    "print(\"=\" * 60)\n",
    "print(content[:2000])\n",
    "if len(content) > 2000:\n",
    "    print(f\"\\n... ({len(content) - 2000:,} more chars)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. LLM Narration Adaptation\n",
    "\n",
    "Send a section to the LLM to produce narration-ready text with pause markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Adapt one section to narration\n",
    "#     Uses the section selected in 2.2 above.\n",
    "\n",
    "section = Section.from_content(title, content, language=config.narration.target_lang)\n",
    "\n",
    "print(f\"Adapting: {section.title}\")\n",
    "print(f\"  has_table={section.has_table}, has_list={section.has_list}\")\n",
    "print(f\"  LLM: {type(config.llm).__name__} / {config.llm.model}\")\n",
    "print(\"Generating narration...\\n\")\n",
    "\n",
    "t0 = time.time()\n",
    "narration = adapt_narration_section(\n",
    "    section,\n",
    "    llm=config.llm,\n",
    "    source_lang=config.narration.source_lang,\n",
    "    target_lang=config.narration.target_lang,\n",
    ")\n",
    "elapsed = time.time() - t0\n",
    "\n",
    "print(f\"Narration generated in {elapsed:.1f}s\")\n",
    "print(f\"Output: {len(narration):,} chars, {len(narration.split())} words\")\n",
    "print(\"=\" * 60)\n",
    "print(narration[:3000])\n",
    "if len(narration) > 3000:\n",
    "    print(f\"\\n... ({len(narration) - 3000:,} more chars)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. TTS Rendering\n",
    "\n",
    "Render the narration to audio with the configured TTS backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Load TTS model and render\n",
    "\n",
    "print(f\"Loading {type(config.tts).__name__} model...\")\n",
    "t0 = time.time()\n",
    "model = load_tts_model(config.tts)\n",
    "print(f\"Model loaded in {time.time() - t0:.1f}s\")\n",
    "\n",
    "print(f\"\\nRendering {len(narration.split())} words...\")\n",
    "t0 = time.time()\n",
    "audio = render_section(narration, config.tts, model=model)\n",
    "render_time = time.time() - t0\n",
    "\n",
    "print(f\"Render time: {render_time:.1f}s\")\n",
    "play(audio, label=\"Audiobook\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Backend Comparison\n",
    "\n",
    "Load all three TTS backends and compare the same narration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Load Kokoro + Chatterbox\n",
    "backends = {\n",
    "    \"Kokoro\": KokoroTTS(voices=(\"af_heart\",), speed=0.95),\n",
    "    \"Chatterbox\": ChatterboxTTS(),\n",
    "}\n",
    "\n",
    "models = {}\n",
    "for name, tts_cfg in backends.items():\n",
    "    print(f\"{name}: {tts_cfg}\")\n",
    "    t0 = time.time()\n",
    "    models[name] = load_tts_model(tts_cfg)\n",
    "    print(f\"  Loaded in {time.time() - t0:.1f}s\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ql0zxmdbgb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Render the same narration with all backends\n",
    "\n",
    "results = {}\n",
    "for name, tts_cfg in backends.items():\n",
    "    print(f\"Rendering with {name}...\")\n",
    "    t0 = time.time()\n",
    "    audio_out = render_section(narration, tts_cfg, model=models[name])\n",
    "    elapsed = time.time() - t0\n",
    "    results[name] = (audio_out, elapsed)\n",
    "\n",
    "# Stats table\n",
    "print(f\"\\n{'Metric':<20}\", end=\"\")\n",
    "for name in results:\n",
    "    print(f\" {name:>16}\", end=\"\")\n",
    "print()\n",
    "print(f\"{'-' * 20}\", end=\"\")\n",
    "for _ in results:\n",
    "    print(f\" {'-' * 16}\", end=\"\")\n",
    "print()\n",
    "\n",
    "print(f\"{'Render time (s)':<20}\", end=\"\")\n",
    "for audio_out, elapsed in results.values():\n",
    "    print(f\" {elapsed:>16.1f}\", end=\"\")\n",
    "print()\n",
    "\n",
    "print(f\"{'Audio duration (s)':<20}\", end=\"\")\n",
    "for audio_out, elapsed in results.values():\n",
    "    print(f\" {len(audio_out) / SAMPLE_RATE:>16.1f}\", end=\"\")\n",
    "print()\n",
    "\n",
    "print(f\"{'Samples':<20}\", end=\"\")\n",
    "for audio_out, elapsed in results.values():\n",
    "    print(f\" {len(audio_out):>16,}\", end=\"\")\n",
    "print()\n",
    "\n",
    "# Play each\n",
    "for name, (audio_out, elapsed) in results.items():\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    play(audio_out)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "long-form-tts (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
