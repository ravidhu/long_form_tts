{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dialogue Prompt Testing\n",
    "\n",
    "Interactive workbench for testing podcast dialogue prompts in isolation — without running the full pipeline.\n",
    "\n",
    "**What you can do here:**\n",
    "- Walk through outline, dialogue segment, and intro/outro generation step by step\n",
    "- Measure quality metrics (word count vs target, speaker balance, turn lengths)\n",
    "- A/B compare `two_hosts` vs `host_guest` formats side by side\n",
    "- Test custom dialogue prompts with raw `llm_generate()` calls\n",
    "\n",
    "Run cells 0-3 (setup) first, then jump to any section."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Setup — add src/ to path so we can import all packages\nimport sys\nimport re\nimport time\nfrom pathlib import Path\n\nsys.path.insert(0, str(Path.cwd().parent / \"src\"))\n\nfrom shared.providers import (\n    OllamaLLM, MLXLLM,\n    llm_generate, ollama_preflight,\n)\nfrom podcast import (\n    DialogueConfig, PodcastConfig,\n    PodcastOutline, DialogueSegment,\n    generate_outline, generate_dialogue_segment, generate_intro_outro,\n)\nfrom podcast.prompts import (\n    DIALOGUE_SYSTEM_PROMPTS, OUTLINE_SYSTEM_PROMPT,\n    INTRO_PROMPT, OUTRO_PROMPT,\n)\n\nprint(\"Imports OK\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# LLM backend — uncomment the one you want to use\n\n## Ollama (default)\nllm = OllamaLLM(model=\"qwen3:14b\", temperature=0.7)\n\n## MLX (Apple Silicon local)\n# llm = MLXLLM(model=\"Qwen/Qwen3-14B-MLX-4bit\", temperature=0.7)\n\n# Preflight check for Ollama\nif isinstance(llm, OllamaLLM):\n    ollama_preflight(llm)\n    print(f\"Ollama ready: {llm.model} @ {llm.url}\")\nelse:\n    print(f\"Using: {type(llm).__name__} ({llm.model})\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Sample content — auto-discover from previous runs, or use inline fallback\n",
    "OUTPUT_DIR = Path.cwd().parent / \"output\"\n",
    "\n",
    "# Find section .md files from previous pipeline runs\n",
    "section_files = sorted(OUTPUT_DIR.glob(\"*/sections/*.md\")) if OUTPUT_DIR.exists() else []\n",
    "\n",
    "if section_files:\n",
    "    # Use the first available section\n",
    "    sample_path = section_files[0]\n",
    "    sample_content = sample_path.read_text()\n",
    "    sample_title = sample_path.stem.replace(\"_\", \" \").title()\n",
    "    print(f\"Loaded: {sample_path.relative_to(OUTPUT_DIR)}\")\n",
    "    print(f\"Title: {sample_title}\")\n",
    "    print(f\"Length: {len(sample_content):,} chars\")\n",
    "    if len(section_files) > 1:\n",
    "        print(f\"\\n{len(section_files)} sections available — change sample_path above to try others:\")\n",
    "        for f in section_files[:10]:\n",
    "            print(f\"  {f.relative_to(OUTPUT_DIR)}\")\n",
    "        if len(section_files) > 10:\n",
    "            print(f\"  ... and {len(section_files) - 10} more\")\n",
    "else:\n",
    "    # Inline fallback with table, list, and numbers\n",
    "    sample_title = \"Performance Benchmarks\"\n",
    "    sample_content = \"\"\"## Performance Benchmarks\n",
    "\n",
    "The following table summarizes the results across all architectures tested in Q3 2024.\n",
    "\n",
    "| Model        | Params | Accuracy | Latency (ms) | Memory (GB) |\n",
    "|--------------|--------|----------|--------------|-------------|\n",
    "| Baseline CNN | 25.6M  | 87.61%   | 12.3         | 2.1         |\n",
    "| ResNet-50    | 25.6M  | 91.42%   | 18.7         | 3.4         |\n",
    "| ViT-B/16     | 86.6M  | 93.18%   | 24.1         | 6.2         |\n",
    "| MLP-Mixer    | 59.9M  | 89.73%   | 15.9         | 4.8         |\n",
    "\n",
    "Key findings from the evaluation:\n",
    "\n",
    "1. ViT-B/16 achieved the highest accuracy at 93.18%, outperforming the CNN baseline by 5.57 percentage points\n",
    "2. The latency-accuracy tradeoff favors ResNet-50 for production deployments\n",
    "3. MLP-Mixer offers a compelling middle ground — 89.73% accuracy with only 15.9ms latency\n",
    "4. All models were evaluated on the ImageNet-1k validation set (50,000 images)\n",
    "\n",
    "For details see https://arxiv.org/abs/2024.12345 and the full results at github.com/example/benchmarks.\n",
    "\n",
    "The `torch.compile()` optimization reduced inference latency by approximately 30% across all architectures.\n",
    "\"\"\"\n",
    "    print(\"Using inline fallback content (no previous runs found)\")\n",
    "    print(f\"Title: {sample_title}\")\n",
    "    print(f\"Length: {len(sample_content):,} chars\")\n",
    "\n",
    "print(f\"\\nReady — sample_title and sample_content set\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Dialogue Walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 1.1 Configure podcast settings (smaller targets for fast testing)\n",
    "dialogue_config = DialogueConfig(\n",
    "    format=\"two_hosts\",\n",
    "    speaker1_name=\"Alex\",\n",
    "    speaker2_name=\"Sam\",\n",
    "    source_lang=\"en\",\n",
    "    target_lang=\"en\",\n",
    "    segment_target_words=800,  # smaller than production 1200 for faster testing\n",
    ")\n",
    "\n",
    "podcast_config = PodcastConfig(\n",
    "    dialogue=dialogue_config,\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "print(f\"Format:       {dialogue_config.format}\")\n",
    "print(f\"Speakers:     {dialogue_config.speaker1_name} & {dialogue_config.speaker2_name}\")\n",
    "print(f\"Language:     {dialogue_config.source_lang} -> {dialogue_config.target_lang}\")\n",
    "print(f\"Target words: {dialogue_config.segment_target_words} per segment\")\n",
    "print(f\"LLM:          {type(llm).__name__} ({llm.model})\")\n",
    "print()\n",
    "print(f\"Available formats: {list(DIALOGUE_SYSTEM_PROMPTS.keys())}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.2 Outline generation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 1.2 Generate podcast outline from sample sections\n",
    "# Sections are (title, content) tuples\n",
    "sample_sections = [(sample_title, sample_content)]\n",
    "\n",
    "print(f\"Input: {len(sample_sections)} section(s)\")\n",
    "print(\"Generating outline...\\n\")\n",
    "\n",
    "t0 = time.time()\n",
    "outline = generate_outline(sample_sections, podcast_config)\n",
    "outline_elapsed = time.time() - t0\n",
    "\n",
    "print(f\"--- Outline ({outline_elapsed:.1f}s) ---\")\n",
    "print(f\"Title: {outline.title}\")\n",
    "print(f\"Segments: {len(outline.segments)}\")\n",
    "print()\n",
    "print(outline.raw_text)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.3 Dialogue segment generation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 1.3 Generate dialogue for the first section\n",
    "print(f\"Generating dialogue for: {sample_title}\")\n",
    "print(f\"Target: ~{dialogue_config.segment_target_words} words\")\n",
    "print(\"Generating...\\n\")\n",
    "\n",
    "t0 = time.time()\n",
    "segment = generate_dialogue_segment(\n",
    "    section_content=sample_content,\n",
    "    section_title=sample_title,\n",
    "    outline=outline,\n",
    "    segment_index=0,\n",
    "    rolling_summary=\"\",\n",
    "    covered_topics=[],\n",
    "    config=podcast_config,\n",
    ")\n",
    "dialogue_elapsed = time.time() - t0\n",
    "\n",
    "print(f\"--- Dialogue ({dialogue_elapsed:.1f}s) ---\\n\")\n",
    "print(segment.dialogue)\n",
    "print(f\"\\n--- Rolling Summary ---\\n\")\n",
    "print(segment.updated_summary)\n",
    "print(f\"\\n--- Covered Topics ---\\n\")\n",
    "for topic in segment.covered_topics:\n",
    "    print(f\"  - {topic}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 1.4 Dialogue quality metrics\n",
    "dialogue_text = segment.dialogue\n",
    "dialogue_words = len(dialogue_text.split())\n",
    "target = dialogue_config.segment_target_words\n",
    "\n",
    "# Speaker turn analysis\n",
    "s1_turns = re.findall(r\"\\[S1\\](.*?)(?=\\[S[12]\\]|$)\", dialogue_text, re.DOTALL)\n",
    "s2_turns = re.findall(r\"\\[S2\\](.*?)(?=\\[S[12]\\]|$)\", dialogue_text, re.DOTALL)\n",
    "\n",
    "s1_words = [len(t.split()) for t in s1_turns]\n",
    "s2_words = [len(t.split()) for t in s2_turns]\n",
    "\n",
    "s1_total = sum(s1_words)\n",
    "s2_total = sum(s2_words)\n",
    "total_speaker_words = s1_total + s2_total\n",
    "balance = min(s1_total, s2_total) / max(s1_total, s2_total) if max(s1_total, s2_total) > 0 else 0\n",
    "\n",
    "all_turn_words = s1_words + s2_words\n",
    "\n",
    "# Pause counts\n",
    "pauses = len(re.findall(r\"\\[PAUSE\\]\", dialogue_text))\n",
    "\n",
    "print(\"Dialogue Quality Metrics\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Total words:       {dialogue_words:>6,}\")\n",
    "print(f\"Target words:      {target:>6,}\")\n",
    "print(f\"Hit rate:          {dialogue_words / target * 100:>5.0f}%\")\n",
    "print(f\"Generation time:   {dialogue_elapsed:>6.1f}s\")\n",
    "print(f\"Words/sec:         {dialogue_words / dialogue_elapsed:>6.1f}\")\n",
    "print()\n",
    "print(f\"Speaker Balance\")\n",
    "print(f\"  {dialogue_config.speaker1_name} (S1): {len(s1_turns)} turns, {s1_total} words\")\n",
    "print(f\"  {dialogue_config.speaker2_name} (S2): {len(s2_turns)} turns, {s2_total} words\")\n",
    "print(f\"  Balance ratio:   {balance:.2f}\" + (\"  (good)\" if balance > 0.6 else \"  *** imbalanced\"))\n",
    "print()\n",
    "if all_turn_words:\n",
    "    print(f\"Turn Length (words)\")\n",
    "    print(f\"  Average:         {sum(all_turn_words) / len(all_turn_words):>6.0f}\")\n",
    "    print(f\"  Min:             {min(all_turn_words):>6}\")\n",
    "    print(f\"  Max:             {max(all_turn_words):>6}\")\n",
    "print()\n",
    "print(f\"Pauses [PAUSE]:    {pauses}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.5 Intro & outro generation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 1.5 Generate intro and outro\n",
    "all_topics = segment.covered_topics\n",
    "\n",
    "print(\"Generating intro and outro...\\n\")\n",
    "\n",
    "t0 = time.time()\n",
    "intro, outro = generate_intro_outro(outline, all_topics, podcast_config)\n",
    "intro_outro_elapsed = time.time() - t0\n",
    "\n",
    "print(f\"--- Intro ({len(intro.split())} words) ---\\n\")\n",
    "print(intro)\n",
    "print(f\"\\n--- Outro ({len(outro.split())} words) ---\\n\")\n",
    "print(outro)\n",
    "print(f\"\\nGeneration time: {intro_outro_elapsed:.1f}s\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. A/B Comparison"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 2.0 Generic A/B comparison helper\n",
    "\n",
    "def compare_ab(label_a, label_b, run_a, run_b):\n",
    "    \"\"\"Run two callables and display a comparison summary.\n",
    "\n",
    "    Each callable should return (text, elapsed_seconds).\n",
    "    Auto-detects podcast dialogue by [S1]/[S2] presence.\n",
    "    \"\"\"\n",
    "    print(f\"Running A: {label_a}...\")\n",
    "    text_a, time_a = run_a()\n",
    "    print(f\"Running B: {label_b}...\")\n",
    "    text_b, time_b = run_b()\n",
    "\n",
    "    words_a = len(text_a.split())\n",
    "    words_b = len(text_b.split())\n",
    "\n",
    "    pauses_a = text_a.count(\"[PAUSE\")\n",
    "    pauses_b = text_b.count(\"[PAUSE\")\n",
    "\n",
    "    # Speaker balance (auto-detected)\n",
    "    def speaker_balance(text):\n",
    "        s1 = sum(len(t.split()) for t in re.findall(r\"\\[S1\\](.*?)(?=\\[S[12]\\]|$)\", text, re.DOTALL))\n",
    "        s2 = sum(len(t.split()) for t in re.findall(r\"\\[S2\\](.*?)(?=\\[S[12]\\]|$)\", text, re.DOTALL))\n",
    "        if s1 + s2 == 0:\n",
    "            return None\n",
    "        return min(s1, s2) / max(s1, s2)\n",
    "\n",
    "    bal_a = speaker_balance(text_a)\n",
    "    bal_b = speaker_balance(text_b)\n",
    "\n",
    "    # Summary table\n",
    "    print(f\"\\n{'Metric':<20} {'A: ' + label_a:>20} {'B: ' + label_b:>20}\")\n",
    "    print(f\"{'-' * 20} {'-' * 20} {'-' * 20}\")\n",
    "    print(f\"{'Words':<20} {words_a:>20,} {words_b:>20,}\")\n",
    "    print(f\"{'Pauses':<20} {pauses_a:>20} {pauses_b:>20}\")\n",
    "    print(f\"{'Time (s)':<20} {time_a:>20.1f} {time_b:>20.1f}\")\n",
    "    print(f\"{'Words/sec':<20} {words_a / time_a:>20.1f} {words_b / time_b:>20.1f}\")\n",
    "    if bal_a is not None or bal_b is not None:\n",
    "        bal_a_str = f\"{bal_a:.2f}\" if bal_a is not None else \"n/a\"\n",
    "        bal_b_str = f\"{bal_b:.2f}\" if bal_b is not None else \"n/a\"\n",
    "        print(f\"{'Speaker balance':<20} {bal_a_str:>20} {bal_b_str:>20}\")\n",
    "\n",
    "    # Show truncated outputs\n",
    "    max_chars = 2000\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"A: {label_a}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    print(text_a[:max_chars])\n",
    "    if len(text_a) > max_chars:\n",
    "        print(f\"\\n... ({len(text_a) - max_chars:,} chars truncated)\")\n",
    "\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"B: {label_b}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    print(text_b[:max_chars])\n",
    "    if len(text_b) > max_chars:\n",
    "        print(f\"\\n... ({len(text_b) - max_chars:,} chars truncated)\")\n",
    "\n",
    "print(\"compare_ab() defined\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.1 Format comparison"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 2.1 Compare two_hosts vs host_guest podcast formats\n",
    "\n",
    "def run_two_hosts():\n",
    "    cfg = PodcastConfig(\n",
    "        dialogue=DialogueConfig(\n",
    "            format=\"two_hosts\",\n",
    "            speaker1_name=\"Alex\",\n",
    "            speaker2_name=\"Sam\",\n",
    "            segment_target_words=600,\n",
    "        ),\n",
    "        llm=llm,\n",
    "    )\n",
    "    t0 = time.time()\n",
    "    ol = generate_outline([(sample_title, sample_content)], cfg)\n",
    "    seg = generate_dialogue_segment(\n",
    "        sample_content, sample_title, ol, 0, \"\", [], cfg,\n",
    "    )\n",
    "    return seg.dialogue, time.time() - t0\n",
    "\n",
    "def run_host_guest():\n",
    "    cfg = PodcastConfig(\n",
    "        dialogue=DialogueConfig(\n",
    "            format=\"host_guest\",\n",
    "            speaker1_name=\"Alex\",\n",
    "            speaker2_name=\"Dr. Chen\",\n",
    "            segment_target_words=600,\n",
    "        ),\n",
    "        llm=llm,\n",
    "    )\n",
    "    t0 = time.time()\n",
    "    ol = generate_outline([(sample_title, sample_content)], cfg)\n",
    "    seg = generate_dialogue_segment(\n",
    "        sample_content, sample_title, ol, 0, \"\", [], cfg,\n",
    "    )\n",
    "    return seg.dialogue, time.time() - t0\n",
    "\n",
    "compare_ab(\"two_hosts\", \"host_guest\", run_two_hosts, run_host_guest)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Custom Prompt Testing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 3.1 Custom dialogue prompt — edit the system prompt with {speaker1}/{speaker2} placeholders\n",
    "speaker1 = \"Alex\"\n",
    "speaker2 = \"Sam\"\n",
    "\n",
    "custom_dialogue_system = f\"\"\"\\\n",
    "You are writing a podcast dialogue between {speaker1} and {speaker2}.\n",
    "\n",
    "Rules:\n",
    "1. Use [S1] for {speaker1}'s lines and [S2] for {speaker2}'s lines\n",
    "2. Keep it conversational — short turns, natural reactions\n",
    "3. Cover the key points from the source material\n",
    "4. Target approximately 600 words total\n",
    "5. Start with [S1] and alternate naturally\n",
    "\n",
    "Output ONLY the dialogue. No stage directions or commentary.\n",
    "\"\"\"\n",
    "\n",
    "dialogue_user_msg = f\"Topic: {sample_title}\\n\\nSource material:\\n{sample_content}\"\n",
    "\n",
    "print(f\"Speakers: {speaker1} & {speaker2}\")\n",
    "print(\"Generating...\\n\")\n",
    "\n",
    "t0 = time.time()\n",
    "custom_dialogue = llm_generate(custom_dialogue_system, dialogue_user_msg, llm)\n",
    "custom_dialogue_elapsed = time.time() - t0\n",
    "\n",
    "# Quick stats\n",
    "cd_words = len(custom_dialogue.split())\n",
    "cd_s1 = len(re.findall(r\"\\[S1\\]\", custom_dialogue))\n",
    "cd_s2 = len(re.findall(r\"\\[S2\\]\", custom_dialogue))\n",
    "\n",
    "print(f\"--- Output ({custom_dialogue_elapsed:.1f}s, {cd_words} words, {cd_s1}+{cd_s2} turns) ---\\n\")\n",
    "print(custom_dialogue)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3.2 Free-form prompt testing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 3.2 Blank slate — edit both prompts and run\n",
    "system_prompt = \"\"\"You are a helpful assistant.\"\"\"\n",
    "\n",
    "user_message = \"\"\"Summarize the following in 3 bullet points:\n",
    "\n",
    "[paste your content here]\n",
    "\"\"\"\n",
    "\n",
    "print(f\"System: {len(system_prompt)} chars\")\n",
    "print(f\"User: {len(user_message)} chars\")\n",
    "print(\"Generating...\\n\")\n",
    "\n",
    "t0 = time.time()\n",
    "result = llm_generate(system_prompt, user_message, llm)\n",
    "elapsed = time.time() - t0\n",
    "\n",
    "print(f\"--- Output ({elapsed:.1f}s, {len(result.split())} words) ---\\n\")\n",
    "print(result)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}