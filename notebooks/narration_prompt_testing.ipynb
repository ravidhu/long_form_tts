{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Narration Prompt Testing\n",
    "\n",
    "Interactive workbench for testing audiobook narration prompts in isolation — without running the full pipeline.\n",
    "\n",
    "**What you can do here:**\n",
    "- Walk through `adapt_narration_section()` step by step\n",
    "- Measure quality metrics (word ratio, pause density, residual artifacts)\n",
    "- A/B compare language pairs or models side by side\n",
    "- Test custom narration prompts with raw `llm_generate()` calls\n",
    "\n",
    "Run cells 0-3 (setup) first, then jump to any section."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "%load_ext autoreload\n%autoreload 2\n\n# Setup — add src/ to path so we can import all packages\nimport sys\nimport re\nimport time\nfrom pathlib import Path\n\nsys.path.insert(0, str(Path.cwd().parent / \"src\"))\n\nfrom shared.providers import (\n    OllamaLLM, MLXLLM,\n    llm_generate, ollama_preflight,\n)\nfrom shared.markdown_parser import Section\nfrom audiobook import adapt_narration_section, NARRATION_SYSTEM_PROMPT\n\nprint(\"Imports OK\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# LLM backend — uncomment the one you want to use\n\n## Ollama (default)\nllm = OllamaLLM(model=\"qwen3:14b\", temperature=0.7)\n\n## MLX (Apple Silicon local)\n# llm = MLXLLM(model=\"Qwen/Qwen3-14B-MLX-4bit\", temperature=0.7)\n\n# Preflight check for Ollama\nif isinstance(llm, OllamaLLM):\n    ollama_preflight(llm)\n    print(f\"Ollama ready: {llm.model} @ {llm.url}\")\nelse:\n    print(f\"Using: {type(llm).__name__} ({llm.model})\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Sample content — auto-discover from previous runs, or use inline fallback\n",
    "OUTPUT_DIR = Path.cwd().parent / \"output\"\n",
    "\n",
    "# Find section .md files from previous pipeline runs\n",
    "section_files = sorted(OUTPUT_DIR.glob(\"*/sections/*.md\")) if OUTPUT_DIR.exists() else []\n",
    "\n",
    "if section_files:\n",
    "    # Use the first available section\n",
    "    sample_path = section_files[0]\n",
    "    sample_content = sample_path.read_text()\n",
    "    sample_title = sample_path.stem.replace(\"_\", \" \").title()\n",
    "    print(f\"Loaded: {sample_path.relative_to(OUTPUT_DIR)}\")\n",
    "    print(f\"Title: {sample_title}\")\n",
    "    print(f\"Length: {len(sample_content):,} chars\")\n",
    "    if len(section_files) > 1:\n",
    "        print(f\"\\n{len(section_files)} sections available — change sample_path above to try others:\")\n",
    "        for f in section_files[:10]:\n",
    "            print(f\"  {f.relative_to(OUTPUT_DIR)}\")\n",
    "        if len(section_files) > 10:\n",
    "            print(f\"  ... and {len(section_files) - 10} more\")\n",
    "else:\n",
    "    # Inline fallback with table, list, and numbers to exercise narration rules\n",
    "    sample_title = \"Performance Benchmarks\"\n",
    "    sample_content = \"\"\"## Performance Benchmarks\n",
    "\n",
    "The following table summarizes the results across all architectures tested in Q3 2024.\n",
    "\n",
    "| Model        | Params | Accuracy | Latency (ms) | Memory (GB) |\n",
    "|--------------|--------|----------|--------------|-------------|\n",
    "| Baseline CNN | 25.6M  | 87.61%   | 12.3         | 2.1         |\n",
    "| ResNet-50    | 25.6M  | 91.42%   | 18.7         | 3.4         |\n",
    "| ViT-B/16     | 86.6M  | 93.18%   | 24.1         | 6.2         |\n",
    "| MLP-Mixer    | 59.9M  | 89.73%   | 15.9         | 4.8         |\n",
    "\n",
    "Key findings from the evaluation:\n",
    "\n",
    "1. ViT-B/16 achieved the highest accuracy at 93.18%, outperforming the CNN baseline by 5.57 percentage points\n",
    "2. The latency-accuracy tradeoff favors ResNet-50 for production deployments\n",
    "3. MLP-Mixer offers a compelling middle ground — 89.73% accuracy with only 15.9ms latency\n",
    "4. All models were evaluated on the ImageNet-1k validation set (50,000 images)\n",
    "\n",
    "For details see https://arxiv.org/abs/2024.12345 and the full results at github.com/example/benchmarks.\n",
    "\n",
    "The `torch.compile()` optimization reduced inference latency by approximately 30% across all architectures.\n",
    "\"\"\"\n",
    "    print(\"Using inline fallback content (no previous runs found)\")\n",
    "    print(f\"Title: {sample_title}\")\n",
    "    print(f\"Length: {len(sample_content):,} chars\")\n",
    "\n",
    "# Build Section object with structural detection\n",
    "has_table = bool(re.search(r\"\\|.*\\|.*\\|\", sample_content))\n",
    "has_list = bool(re.search(r\"^\\s*[-*\\d]+[.)\\]]?\\s\", sample_content, re.MULTILINE))\n",
    "\n",
    "sample_section = Section(\n",
    "    title=sample_title,\n",
    "    content=sample_content,\n",
    "    has_table=has_table,\n",
    "    has_list=has_list,\n",
    "    language=\"en\",\n",
    ")\n",
    "print(f\"\\nSection flags: has_table={has_table}, has_list={has_list}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Narration Walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# 1.1 Base system prompt\nprint(\"Base system prompt (NARRATION_SYSTEM_PROMPT):\\n\")\nprint(f\"  {len(NARRATION_SYSTEM_PROMPT):,} chars\")\nprint(f\"  First 200 chars: {NARRATION_SYSTEM_PROMPT[:200]}...\")\nprint(\"\\nLanguage instruction is appended dynamically for non-English pairs.\")\nprint(\"Any language pair the LLM can handle is supported.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.2 Narration adaptation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 1.2 Run adapt_narration_section() on sample content\n",
    "source_lang = \"en\"\n",
    "target_lang = \"en\"\n",
    "\n",
    "print(f\"Language pair: {source_lang} -> {target_lang}\")\n",
    "print(f\"Input: {len(sample_section.content):,} chars, \"\n",
    "      f\"has_table={sample_section.has_table}, has_list={sample_section.has_list}\")\n",
    "print(\"Generating...\\n\")\n",
    "\n",
    "t0 = time.time()\n",
    "narration_output = adapt_narration_section(\n",
    "    sample_section, llm,\n",
    "    source_lang=source_lang,\n",
    "    target_lang=target_lang,\n",
    ")\n",
    "narration_elapsed = time.time() - t0\n",
    "\n",
    "print(f\"--- Output ({narration_elapsed:.1f}s) ---\\n\")\n",
    "print(narration_output)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 1.3 Narration quality metrics\n",
    "input_words = len(sample_section.content.split())\n",
    "output_words = len(narration_output.split())\n",
    "ratio = output_words / input_words if input_words else 0\n",
    "\n",
    "pause_short = narration_output.count(\"[PAUSE_SHORT]\")\n",
    "pause_medium = narration_output.count(\"[PAUSE_MEDIUM]\")\n",
    "pause_long = narration_output.count(\"[PAUSE_LONG]\")\n",
    "total_pauses = pause_short + pause_medium + pause_long\n",
    "\n",
    "# Residual artifact checks\n",
    "residual_md = len(re.findall(r\"[*_#`|]\", narration_output))\n",
    "residual_urls = len(re.findall(r\"https?://\\S+\", narration_output))\n",
    "residual_code = len(re.findall(r\"`[^`]+`\", narration_output))\n",
    "\n",
    "print(\"Narration Quality Metrics\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Input words:       {input_words:>6,}\")\n",
    "print(f\"Output words:      {output_words:>6,}\")\n",
    "print(f\"Ratio (out/in):    {ratio:>6.2f}x\")\n",
    "print(f\"Generation time:   {narration_elapsed:>6.1f}s\")\n",
    "print(f\"Words/sec:         {output_words / narration_elapsed:>6.1f}\")\n",
    "print()\n",
    "print(f\"Pauses: {total_pauses} total\")\n",
    "print(f\"  [PAUSE_SHORT]:   {pause_short}\")\n",
    "print(f\"  [PAUSE_MEDIUM]:  {pause_medium}\")\n",
    "print(f\"  [PAUSE_LONG]:    {pause_long}\")\n",
    "print()\n",
    "print(f\"Residual artifacts:\")\n",
    "print(f\"  Markdown chars:  {residual_md}\" + (\"  *** check output\" if residual_md > 5 else \"\"))\n",
    "print(f\"  URLs:            {residual_urls}\" + (\"  *** should be 0\" if residual_urls else \"\"))\n",
    "print(f\"  Code spans:      {residual_code}\" + (\"  *** should be 0\" if residual_code else \"\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. A/B Comparison"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 2.0 Generic A/B comparison helper\n",
    "\n",
    "def compare_ab(label_a, label_b, run_a, run_b):\n",
    "    \"\"\"Run two callables and display a comparison summary.\n",
    "\n",
    "    Each callable should return (text, elapsed_seconds).\n",
    "    \"\"\"\n",
    "    print(f\"Running A: {label_a}...\")\n",
    "    text_a, time_a = run_a()\n",
    "    print(f\"Running B: {label_b}...\")\n",
    "    text_b, time_b = run_b()\n",
    "\n",
    "    words_a = len(text_a.split())\n",
    "    words_b = len(text_b.split())\n",
    "\n",
    "    pauses_a = text_a.count(\"[PAUSE\")\n",
    "    pauses_b = text_b.count(\"[PAUSE\")\n",
    "\n",
    "    # Summary table\n",
    "    print(f\"\\n{'Metric':<20} {'A: ' + label_a:>20} {'B: ' + label_b:>20}\")\n",
    "    print(f\"{'-' * 20} {'-' * 20} {'-' * 20}\")\n",
    "    print(f\"{'Words':<20} {words_a:>20,} {words_b:>20,}\")\n",
    "    print(f\"{'Pauses':<20} {pauses_a:>20} {pauses_b:>20}\")\n",
    "    print(f\"{'Time (s)':<20} {time_a:>20.1f} {time_b:>20.1f}\")\n",
    "    print(f\"{'Words/sec':<20} {words_a / time_a:>20.1f} {words_b / time_b:>20.1f}\")\n",
    "\n",
    "    # Show truncated outputs\n",
    "    max_chars = 2000\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"A: {label_a}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    print(text_a[:max_chars])\n",
    "    if len(text_a) > max_chars:\n",
    "        print(f\"\\n... ({len(text_a) - max_chars:,} chars truncated)\")\n",
    "\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"B: {label_b}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    print(text_b[:max_chars])\n",
    "    if len(text_b) > max_chars:\n",
    "        print(f\"\\n... ({len(text_b) - max_chars:,} chars truncated)\")\n",
    "\n",
    "print(\"compare_ab() defined\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.1 Language pairs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 2.1 Compare en->en vs en->fr narration\n",
    "\n",
    "def run_en_en():\n",
    "    t0 = time.time()\n",
    "    text = adapt_narration_section(sample_section, llm, source_lang=\"en\", target_lang=\"en\")\n",
    "    return text, time.time() - t0\n",
    "\n",
    "def run_en_fr():\n",
    "    t0 = time.time()\n",
    "    text = adapt_narration_section(sample_section, llm, source_lang=\"en\", target_lang=\"fr\")\n",
    "    return text, time.time() - t0\n",
    "\n",
    "compare_ab(\"en -> en\", \"en -> fr\", run_en_en, run_en_fr)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.2 Different models"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# 2.2 Compare two models — edit llm_b to test your preferred alternative\n\n## Ollama alternative\nllm_b = OllamaLLM(model=\"qwen3:8b\", temperature=0.7)\n\n## MLX alternative\n# llm_b = MLXLLM(model=\"Qwen/Qwen3-14B-MLX-4bit\", temperature=0.7)\n\n# Preflight for Ollama model B\nif isinstance(llm_b, OllamaLLM):\n    ollama_preflight(llm_b)\n\nlabel_a = f\"{type(llm).__name__}({llm.model})\"\nlabel_b = f\"{type(llm_b).__name__}({llm_b.model})\"\n\ndef run_model_a():\n    t0 = time.time()\n    text = adapt_narration_section(sample_section, llm, source_lang=\"en\", target_lang=\"en\")\n    return text, time.time() - t0\n\ndef run_model_b():\n    t0 = time.time()\n    text = adapt_narration_section(sample_section, llm_b, source_lang=\"en\", target_lang=\"en\")\n    return text, time.time() - t0\n\ncompare_ab(label_a, label_b, run_model_a, run_model_b)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Custom Prompt Testing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 3.1 Custom narration prompt — edit the system prompt below and run\n",
    "custom_system = \"\"\"\\\n",
    "You are a narrator adapting written content for audiobook narration.\n",
    "\n",
    "Rules:\n",
    "1. Convert all tables into clear comparative statements\n",
    "2. Expand abbreviations on first occurrence\n",
    "3. Write numbers as spoken words (\"twenty-five\" not \"25\")\n",
    "4. Remove all markdown formatting, URLs, and code references\n",
    "5. Add [PAUSE_SHORT], [PAUSE_MEDIUM], [PAUSE_LONG] for natural pacing\n",
    "6. Use an engaging, authoritative tone\n",
    "\n",
    "Output ONLY the narration text. No commentary.\n",
    "\"\"\"\n",
    "\n",
    "user_message = f\"Section: {sample_title}\\n\\nContent:\\n{sample_content}\"\n",
    "\n",
    "print(f\"System prompt: {len(custom_system)} chars\")\n",
    "print(f\"User message: {len(user_message)} chars\")\n",
    "print(\"Generating...\\n\")\n",
    "\n",
    "t0 = time.time()\n",
    "custom_narration = llm_generate(custom_system, user_message, llm)\n",
    "custom_elapsed = time.time() - t0\n",
    "\n",
    "print(f\"--- Output ({custom_elapsed:.1f}s, {len(custom_narration.split())} words) ---\\n\")\n",
    "print(custom_narration)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3.2 Free-form prompt testing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 3.2 Blank slate — edit both prompts and run\n",
    "system_prompt = \"\"\"You are a helpful assistant.\"\"\"\n",
    "\n",
    "user_message = \"\"\"Summarize the following in 3 bullet points:\n",
    "\n",
    "[paste your content here]\n",
    "\"\"\"\n",
    "\n",
    "print(f\"System: {len(system_prompt)} chars\")\n",
    "print(f\"User: {len(user_message)} chars\")\n",
    "print(\"Generating...\\n\")\n",
    "\n",
    "t0 = time.time()\n",
    "result = llm_generate(system_prompt, user_message, llm)\n",
    "elapsed = time.time() - t0\n",
    "\n",
    "print(f\"--- Output ({elapsed:.1f}s, {len(result.split())} words) ---\\n\")\n",
    "print(result)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}