{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOC Analysis Explorer\n",
    "\n",
    "Interactive walkthrough of the TOC analysis pipeline (`src/pdf_parser/`).  \n",
    "Run each cell to see how a PDF's structure is extracted, classified, and split into sections.\n",
    "\n",
    "The notebook follows the same flow as `_get_toc()` → `resolve_content_pages()` → `resolve_content_sections()` in `resolve_content.py`.\n",
    "\n",
    "See [docs/toc_analysis.md](../docs/toc_analysis.md) for the full reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup — add src/ to path so we can import pdf_parser\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your PDF — uncomment one line or set your own path\n",
    "INPUTS = Path.cwd().parent / \"inputs\"\n",
    "\n",
    "## Books\n",
    "# PDF_PATH = str(INPUTS / \"little-book-deep-learning.pdf\")                      # #15 — Rich 3-level TOC, front matter, 185 pages\n",
    "PDF_PATH = str(INPUTS / \"2019BurkovTheHundred-pageMachineLearning.pdf\")     # #4  — Sparse TOC (~3% coverage), falls back to infer_toc\n",
    "# PDF_PATH = str(INPUTS / \"Book-all-in-one.pdf\")                              # #6  — Math Foundation of RL, deep TOC, 450 pages\n",
    "\n",
    "## Research papers — embedded TOC\n",
    "# PDF_PATH = str(INPUTS / \"attention-is-all-you-need.pdf\")                    # #7  — Transformer, 15 pages, 67% coverage\n",
    "# PDF_PATH = str(INPUTS / \"gpt3-few-shot-learners.pdf\")                      # #9  — GPT-3, 75 pages, 84% coverage\n",
    "\n",
    "## Research papers — no embedded TOC (pure infer_toc)\n",
    "# PDF_PATH = str(INPUTS / \"alexnet.pdf\")                                      # #13 — AlexNet, 9 pages, zero bookmarks\n",
    "# PDF_PATH = str(INPUTS / \"adam-optimizer.pdf\")                               # #14 — Adam, 15 pages, zero bookmarks\n",
    "\n",
    "print(f\"PDF: {PDF_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Embedded TOC extraction\n",
    "\n",
    "Reads the PDF's built-in bookmarks via `pymupdf.get_toc()`.  \n",
    "Each entry has a **level** (nesting depth), **title**, and **page** (0-indexed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from shared.pdf_parser.extract_toc import extract_toc\nimport pymupdf\n\nentries = extract_toc(PDF_PATH)\ndoc = pymupdf.open(PDF_PATH)\ntotal_pages = len(doc)\ndoc.close()\n\nprint(f\"Total pages: {total_pages}\")\nprint(f\"Embedded entries: {len(entries)}\")\nprint(f\"Levels found: {sorted(set(e.level for e in entries)) if entries else 'none'}\")\nprint()\n\nif entries:\n    for e in entries:\n        indent = \"  \" * (e.level - 1)\n        print(f\"  {indent}L{e.level}  p.{e.page:<4} {e.title}\")\nelse:\n    print(\"  No embedded TOC — will fall back to Docling inference.\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Page coverage check\n",
    "\n",
    "The coverage ratio determines whether to trust the embedded TOC or fall back to Docling inference.  \n",
    "**Formula**: `max_page / total_pages` — if below `min_coverage` (default 30%), the embedded TOC is considered too sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_coverage = 0.3  # configurable — try changing this\n",
    "\n",
    "if entries:\n",
    "    max_page = max(e.page for e in entries)\n",
    "    coverage = max_page / total_pages\n",
    "    passes = max_page >= total_pages * min_coverage\n",
    "\n",
    "    print(f\"Highest page referenced: {max_page} / {total_pages - 1}\")\n",
    "    print(f\"Coverage: {coverage:.1%}\")\n",
    "    print(f\"Threshold: {min_coverage:.0%}\")\n",
    "    print(f\"Result: {'PASS — using embedded TOC' if passes else 'FAIL — falling back to Docling inference'}\")\n",
    "else:\n",
    "    print(\"No embedded entries — coverage is 0%, falling back to Docling inference.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Inferred TOC (Docling fallback)\n",
    "\n",
    "When the embedded TOC is missing or too sparse, `_get_toc()` falls back to `infer_toc()`.  \n",
    "Docling AI layout analysis detects headings (`TITLE`/`SECTION_HEADER`), then hierarchy is assigned via:\n",
    "1. **Section numbering** (e.g. \"3.1.2\" → level = dot-count + 1) when majority of headings are numbered\n",
    "2. **Font-size fallback** — looks up each heading's font size in PyMuPDF, largest = L1, rest = L2\n",
    "\n",
    "This cell always runs inference for exploration, even if the embedded TOC passed coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from shared.pdf_parser.infer_toc import infer_toc\n\ninfer_toc.cache_clear()  # clear cache so we get fresh results\ninferred = infer_toc(PDF_PATH)\n\nif inferred:\n    print(f\"Inferred {len(inferred)} entries:\")\n    print()\n    for e in inferred:\n        indent = \"  \" * (e.level - 1)\n        print(f\"  {indent}L{e.level}  p.{e.page:<4} {e.title}\")\nelse:\n    print(\"No headings inferred.\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. TOC source decision (`_get_toc`)\n",
    "\n",
    "This mirrors the logic in `resolve_content._get_toc()`:  \n",
    "embedded TOC is used if coverage passes; otherwise falls back to inferred TOC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from shared.pdf_parser.resolve_content import _get_toc\n\ntoc_entries = _get_toc(PDF_PATH, min_coverage=min_coverage)\n\n# Determine which source was chosen (same logic as _get_toc)\nif entries and max(e.page for e in entries) >= total_pages * min_coverage:\n    source = \"embedded\"\nelse:\n    source = \"inferred (Docling)\"\n\nprint(f\"Source: {source}\")\nprint(f\"Entries: {len(toc_entries)}\")\nprint(f\"Levels: {sorted(set(e.level for e in toc_entries))}\")\nprint()\nfor e in toc_entries:\n    indent = \"  \" * (e.level - 1)\n    print(f\"  {indent}L{e.level}  p.{e.page:<4} {e.title}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Entry classification\n",
    "\n",
    "Each TOC entry is classified as **front matter**, **back matter**, **preamble**, or **content**.  \n",
    "Front and back matter are skipped; preamble and content are included in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from shared.pdf_parser.classify_entry import classify_entry\n\n# Classify entries from step 4 (toc_entries already set by _get_toc)\nfor e in toc_entries:\n    e.kind = classify_entry(e)\n\n# Color-code by kind\nKIND_ICON = {\"front\": \"⊘\", \"back\": \"⊘\", \"preamble\": \"◈\", \"content\": \"●\"}\n\nprint(f\"Using: {source} TOC ({len(toc_entries)} entries)\")\nprint()\nfor e in toc_entries:\n    indent = \"  \" * (e.level - 1)\n    icon = KIND_ICON.get(e.kind, \"?\")\n    print(f\"  {icon} {indent}L{e.level}  p.{e.page:<4} {e.title:<40} [{e.kind}]\")\n\nprint()\nprint(\"Legend: ● content  ◈ preamble  ⊘ skipped (front/back matter)\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Content range resolution\n",
    "\n",
    "Determines the page range of actual content by finding the first preamble/content entry  \n",
    "and the last entry before back matter begins. Mirrors `resolve_content_pages()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from shared.pdf_parser.resolve_content import resolve_content_pages\n\ncr = resolve_content_pages(PDF_PATH, min_coverage=min_coverage)\n\ncontent_pages = cr.end_page - cr.start_page + 1\n\nprint(f\"Content range: pages {cr.start_page}–{cr.end_page} ({content_pages} of {cr.total_pages} pages)\")\nprint()\nif cr.skipped_front:\n    print(\"Skipped front matter:\")\n    for s in cr.skipped_front:\n        print(f\"  {s}\")\nelse:\n    print(\"No front matter skipped.\")\nprint()\nif cr.skipped_back:\n    print(\"Skipped back matter:\")\n    for s in cr.skipped_back:\n        print(f\"  {s}\")\nelse:\n    print(\"No back matter skipped.\")\n\nprint()\npct = content_pages / cr.total_pages * 100\nbar = \"░\" * cr.start_page + \"█\" * content_pages + \"░\" * (cr.total_pages - cr.end_page - 1)\n# Scale bar to ~60 chars\nscale = max(1, cr.total_pages // 60)\nbar_scaled = bar[::scale]\nprint(f\"Page map: [{bar_scaled}]\")\nprint(f\"          █ = content ({pct:.0f}%)  ░ = skipped\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Section splitting\n",
    "\n",
    "Splits the content range into sections using `resolve_content_sections()`.  \n",
    "Try different `max_level` values to see coarser vs finer splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from shared.pdf_parser.resolve_content import resolve_content_sections\n\nmax_level = 1       # try 1, 2, 3\nmax_tokens = 24000  # set to None to disable auto-subdivision\n\nsections = resolve_content_sections(PDF_PATH, max_level=max_level, max_tokens=max_tokens, min_coverage=min_coverage)\n\nprint(f\"max_level={max_level}, max_tokens={max_tokens}\")\nprint(f\"Sections: {len(sections)}\")\nprint()\n\ndoc = pymupdf.open(PDF_PATH)\nfor i, s in enumerate(sections):\n    pages = s.end_page - s.start_page + 1\n    # Estimate tokens\n    chars = sum(len(doc[p].get_text()) for p in range(s.start_page, s.end_page + 1))\n    tokens = chars // 4\n    indent = \"  \" * (s.level - 1)\n    print(f\"  {i:>2}. {indent}L{s.level}  p.{s.start_page}–{s.end_page} ({pages:>3} pp, ~{tokens:,} tok)  {s.title}\")\ndoc.close()\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Compare splitting granularity\n",
    "\n",
    "Side-by-side comparison of `max_level=1` vs `max_level=2` to see how splitting changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = resolve_content_sections(PDF_PATH, max_level=1, max_tokens=max_tokens, min_coverage=min_coverage)\n",
    "s2 = resolve_content_sections(PDF_PATH, max_level=2, max_tokens=max_tokens, min_coverage=min_coverage)\n",
    "\n",
    "print(f\"{'max_level=1':>40}  │  max_level=2\")\n",
    "print(f\"{'─' * 40}──┼──{'─' * 40}\")\n",
    "\n",
    "max_rows = max(len(s1), len(s2))\n",
    "for i in range(max_rows):\n",
    "    left = f\"p.{s1[i].start_page}–{s1[i].end_page} {s1[i].title}\" if i < len(s1) else \"\"\n",
    "    right = f\"p.{s2[i].start_page}–{s2[i].end_page} {s2[i].title}\" if i < len(s2) else \"\"\n",
    "    print(f\"  {left:>38}  │  {right}\")\n",
    "\n",
    "print(f\"\\n  {'Sections: ' + str(len(s1)):>38}  │  Sections: {len(s2)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdf-to-audio (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}